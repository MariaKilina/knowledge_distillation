# Дистилляция знаний сверточных нейронных сетей
На примере решения задачи классификации изображений сравниваются три алгоритма дистилляции знаний: 
1. [Классическая дистилляция](https://arxiv.org/abs/1503.02531) "учитель-ученик"
2. [Дистилляция с использованием двух ассистентов учителя](https://arxiv.org/abs/2009.08825)
3. [Самодистилляция](https://arxiv.org/abs/1905.08094)

Для наглядной разницы в точности моделей используются следующие архитектуры:

<img width="800" height="550" alt="resnet (1)" src="https://github.com/user-attachments/assets/dfedf909-4143-4656-a3ba-0681c1904f38" />
